# -*- coding: utf-8 -*-
"""Bert_NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ore1VTXPby-DFL2YnvhhaKBhsQhR9huh
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import nltk
import csv
from nltk.tokenize import sent_tokenize
from IPython.display import clear_output
nltk.download('punkt')

def sentence_processing (sentence):
    # sentence = sentence.replace("/", " ")
    # sentence = sentence.replace("-", " ")
    # sentence = sentence.replace("(", " ")
    # sentence = sentence.replace(")", " ")
    # sentence = sentence.replace("#", " ")
    # sentence = sentence.replace("..", " .")
    # sentence = sentence.replace("...", "  .")
    # sentence = sentence.replace("[", " ")
    # sentence = sentence.replace("]", " ")
    # sentence = sentence.replace("*", " ")
    # sentence = sentence.replace("\"", " ")
        
    sentence_map, temp_1 = {},{}
    temp = sentence.split("||")    
    start, end, event =  temp[0].split(" ")[-2], temp[0].split(" ")[-1], temp[0].split(" ")[:-2]    
    for i in range(1, len(temp)):        
        key, value = temp[i].split("=")
        temp_1[key] = value.replace('\"', "")
    sentence_id = start.split(":")[0]
    start = start.split(":")[1].replace('\"', "")
    end = end.split(":")[1].replace('\"', "")
    event_1, type_of_event = [event[0].split("=")[1]], event[0].split("=")[0]
    event_1[0] = event_1[0].replace('\"', "")    
    for i in range(1, len(event)):
        event_1.append(event[i].replace('\"', ""))    
    temp_1["start"] = start
    temp_1["end"] = end
    temp_1["event"] = " ".join(event_1)
    sentence_map[sentence_id] = temp_1
    if type_of_event == "EVENT" or "TIMEX3" or "SECTIME":
        return sentence_map   
    else: 
        return 0

import re
import string
def preprocess1(x):
    y=re.sub('\\[(.*?)\\]','',x)
    y=re.sub('dr\.','doctor',y)
    y=re.sub('m\.d\.','md',y)
    
    
    # remove, digits, spaces
    # y = y.translate(str.maketrans("", "", string.digits))
    y = " ".join(y.split())
    return y

def preprocessing(df_notes): 
    df_notes['Word']=df_notes['Word'].fillna(' ')
    df_notes['Word']=df_notes['Word'].str.replace('\n',' ')
    df_notes['Word']=df_notes['Word'].str.replace('\r',' ')
    df_notes['Word']=df_notes['Word'].apply(str.strip)
    df_notes['Word']=df_notes['Word'].str.lower()

    df_notes['Word']=df_notes['Word'].apply(lambda x: preprocess1(x))
    
    return df_notes

def file_process (filename, filename_ann, labels):
    path = "/content/drive/My Drive/NLP Project/Dropbox files/training data/"
    #path = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
    f_text = open(os.path.expanduser(path + filename)).read()
    f_ann = open(os.path.expanduser(path + filename_ann)).read()
    sent_tokenize_list = sent_tokenize(f_ann)
    sentences_individual = f_ann.split("\n")[:-1]
    sentence_mapping = []
    for i in sentences_individual:
        output = sentence_processing(i)
        if output == 0:
            print(i)
        else:
            sentence_mapping.append(output)

    final_sentence_mapping = []
    for sentence in sentence_mapping:    
        for key, value in sentence.items():
            if value["type"] in labels:
                final_sentence_mapping.append(sentence)

    all_bio_tags = []
    for i in list(labels):
        all_bio_tags.append("i-"+i)
        all_bio_tags.append("b-"+i)
    all_bio_tags.append("O")

    sentence_word_tag_map = {}
    for i in final_sentence_mapping:
        for key, value in i.items():
            sentence = int(key) - 1
            start,end = int(value["start"]), int(value["end"])
            if end - start == 0:
                sentence_word_tag_map[(sentence, start)] = "b-" + value["type"]
            else:
                temp = value["event"].split(" ")
                for i in range(len(temp)):
                    if i == 0:                    
                        sentence_word_tag_map[(sentence,start+i)] = "b-" + value["type"]
                    else:
                        sentence_word_tag_map[(sentence,start+i)] = "i-" + value["type"]

    sentence_word_tag_map = dict(sorted(sentence_word_tag_map.items(), key = lambda x: x[0][0]))


    input_tokens = []
    text = f_text.split ("\n")
    for i in range(len(text)):
        temp = text[i].split(" ")
        for j in range(len(temp)):
            temp_1 = [0] * 3
            temp_1[0] = "Sentence: "+str(i)
            temp_1[1] = temp[j]
            if (i,j) in sentence_word_tag_map:
                temp_1[2] = sentence_word_tag_map[(i,j)]
            else:
                temp_1[2] = "O"
            input_tokens.append(temp_1)
    #also return number of sentences
    return input_tokens, len(sentences_individual)

def process_input_tokens(input_tokens, offset):
    #check count_file. If count file is greater than 1, then make the changes.
    new = []
    for entry in input_tokens:
        new_sentence_number = int(entry[0][entry[0].find(" ") +1 :]) + offset
        new.append(["Sentence: " + str(new_sentence_number) , entry[1], entry[2]])
    return new

labels = {'EVIDENTIAL','OCCURRENCE','PROBLEM', 'TEST', 'TREATMENT', 'CLINICAL_DEPT', "DATE",
                            "DURATION", "DISCHARGE", "ADMISSION", "TIME", "FREQUENCY"}
directory = "/content/drive/My Drive/NLP Project/Dropbox files/training data/"
#directory = os.fsencode(directory)
count_file, count_sentences, input_tokens_final = 0, 0, []
for file in os.listdir(directory):
    #print (file)
    filename = os.fsdecode(file)
    #print ("niorhgr", filename)
    if filename.endswith(".txt"):
        count_file += 1
        filename_ann = filename[:filename.find(".")] + ".xml.extent"
        input_tokens, offset = file_process(filename, filename_ann, labels)        
        input_tokens_final += process_input_tokens(input_tokens, count_sentences)
        count_sentences += offset

import pandas as pd
import numpy as np
from tqdm import tqdm, trange

data = pd.DataFrame(input_tokens_final,columns=['Sentence#','Word','Tag'])
# data = data.sample(frac=1)
#data = preprocessing(data)
data

class SentenceGetter(object):
    
    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: [(w,t) for w,t in zip(s["Word"].values.tolist(),
                                                           s["Tag"].values.tolist())]
        self.grouped = self.data.groupby("Sentence#").apply(agg_func)
        self.sentences = [s for s in self.grouped]
    
    def get_next(self):
        try:
            s = self.grouped["Sentence: {}".format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None

getter = SentenceGetter(data)

sentences = [" ".join([s[0] for s in sent]) for sent in getter.sentences]
sentences[0]

labels = [[s[1] for s in sent] for sent in getter.sentences]
print(labels[0])

tags_vals = list(set(data["Tag"].values))
tag2idx = {t: i for i, t in enumerate(tags_vals)}

pip install pytorch-pretrained-bert==0.4.0

import torch
from torch.optim import Adam
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from pytorch_pretrained_bert import BertTokenizer, BertConfig
from pytorch_pretrained_bert import BertForTokenClassification, BertAdam

MAX_LEN = 75
bs = 32

torch.cuda.is_available()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()

torch.cuda.get_device_name(0)

torch.cuda.empty_cache()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]

input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                          maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
input_ids

tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],
                     maxlen=MAX_LEN, value=tag2idx["O"], padding="post",
                     dtype="long", truncating="post")
tags

attention_masks = [[float(i>0) for i in ii] for ii in input_ids]

tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, 
                                                            random_state=2018, test_size=0.1)
tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=2018, test_size=0.1)

tr_inputs = torch.tensor(tr_inputs).to(torch.int64)
val_inputs = torch.tensor(val_inputs).to(torch.int64)
tr_tags = torch.tensor(tr_tags).to(torch.int64)
val_tags = torch.tensor(val_tags).to(torch.int64)
tr_masks = torch.tensor(tr_masks).to(torch.int64)
val_masks = torch.tensor(val_masks).to(torch.int64)

train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)

valid_data = TensorDataset(val_inputs, val_masks, val_tags)
valid_sampler = SequentialSampler(valid_data)
valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)

model = BertForTokenClassification.from_pretrained("bert-base-uncased", num_labels=len(tag2idx))

model.cuda();

FULL_FINETUNING = True
if FULL_FINETUNING:
    param_optimizer = list(model.named_parameters())
    # print (param_optimizer) *refer later*
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
else:
    param_optimizer = list(model.classifier.named_parameters()) 
    print (param_optimizer)
    optimizer_grouped_parameters = [{"params": [p for n, p in param_optimizer]}]
optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)

pip install seqeval

import seqeval
from seqeval.metrics import f1_score

def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=2).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

torch.cuda.empty_cache() 
epochs = 20
max_grad_norm = 1.0

for _ in trange(epochs, desc="Epoch"):
    # TRAIN loop
    model.train()
    tr_loss = 0
    nb_tr_examples, nb_tr_steps = 0, 0
    for step, batch in enumerate(train_dataloader):
        # add batch to gpu
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        # forward pass
        loss = model(b_input_ids, token_type_ids=None,
                    attention_mask=b_input_mask, labels=b_labels)
    # backward pass
        loss.backward()
        # track train loss
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1
        # gradient clipping
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        # update parameters
        optimizer.step()
        model.zero_grad()
    # print train loss per epoch
    print("Train loss: {}".format(tr_loss/nb_tr_steps))
    # VALIDATION on validation set
    model.eval()
    eval_loss, eval_accuracy = 0, 0
    nb_eval_steps, nb_eval_examples = 0, 0
    predictions , true_labels = [], []
    for batch in valid_dataloader:
        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        
        with torch.no_grad():
            tmp_eval_loss = model(b_input_ids, token_type_ids=None,
                                  attention_mask=b_input_mask, labels=b_labels)
            logits = model(b_input_ids, token_type_ids=None,
                          attention_mask=b_input_mask)
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
        true_labels.append(label_ids)
        
        tmp_eval_accuracy = flat_accuracy(logits, label_ids)
        
        eval_loss += tmp_eval_loss.mean().item()
        eval_accuracy += tmp_eval_accuracy
        
        nb_eval_examples += b_input_ids.size(0)
        nb_eval_steps += 1
    eval_loss = eval_loss/nb_eval_steps
    print("Validation loss: {}".format(eval_loss))
    print("Validation Accuracy: {}".format(eval_accuracy/nb_eval_steps))
    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]
    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]
    print("F1-Score: {}".format(f1_score(pred_tags, valid_tags)))

#EVALUATION ON EVENTS 

def events_sentence_processing (sentence):
    # sentence = sentence.replace("/", " ")
    # sentence = sentence.replace("-", " ")
    # sentence = sentence.replace("(", " ")
    # sentence = sentence.replace(")", " ")
    # sentence = sentence.replace("#", " ")
    # sentence = sentence.replace("..", " .")
    # sentence = sentence.replace("...", "  .")
    # sentence = sentence.replace("[", " ")
    # sentence = sentence.replace("]", " ")
    # sentence = sentence.replace("*", " ")
    # sentence = sentence.replace("\"", " ")
        
    sentence_map, temp_1 = {},{}
    temp = sentence.split("||")    
    start, end, event =  temp[0].split(" ")[-2], temp[0].split(" ")[-1], temp[0].split(" ")[:-2]    
    for i in range(1, len(temp)):        
        key, value = temp[i].split("=")
        temp_1[key] = value.replace('\"', "")
    sentence_id = start.split(":")[0]
    start = start.split(":")[1].replace('\"', "")
    end = end.split(":")[1].replace('\"', "")
    event_1, type_of_event = [event[0].split("=")[1]], event[0].split("=")[0]
    event_1[0] = event_1[0].replace('\"', "")    
    for i in range(1, len(event)):
        event_1.append(event[i].replace('\"', ""))    
    temp_1["start"] = start
    temp_1["end"] = end
    temp_1["event"] = " ".join(event_1)
    sentence_map[sentence_id] = temp_1
    if type_of_event == "EVENT":
        return sentence_map   
    else: 
        return 0




def test_file_process (filename, filename_ann, labels):
    #path = "/content/drive/My Drive/NLP Project/Dropbox files/training data/"
    path = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
    f_text = open(os.path.expanduser(path + filename)).read()
    f_ann = open(os.path.expanduser(path + filename_ann)).read()
    sent_tokenize_list = sent_tokenize(f_ann)
    sentences_individual = f_ann.split("\n")[:-1]
    sentence_mapping = []
    for i in sentences_individual:
        output = events_sentence_processing(i)
        if output != 0:
        #     print(i)
        # else:
            sentence_mapping.append(output)

    final_sentence_mapping = []
    for sentence in sentence_mapping:    
        for key, value in sentence.items():
            if value["type"] in labels:
                final_sentence_mapping.append(sentence)

    all_bio_tags = []
    for i in list(labels):
        all_bio_tags.append("i-"+i)
        all_bio_tags.append("b-"+i)
    all_bio_tags.append("O")

    sentence_word_tag_map = {}
    for i in final_sentence_mapping:
        for key, value in i.items():
            sentence = int(key) - 1
            start,end = int(value["start"]), int(value["end"])
            if end - start == 0:
                sentence_word_tag_map[(sentence, start)] = "b-" + value["type"]
            else:
                temp = value["event"].split(" ")
                for i in range(len(temp)):
                    if i == 0:                    
                        sentence_word_tag_map[(sentence,start+i)] = "b-" + value["type"]
                    else:
                        sentence_word_tag_map[(sentence,start+i)] = "i-" + value["type"]

    sentence_word_tag_map = dict(sorted(sentence_word_tag_map.items(), key = lambda x: x[0][0]))


    input_tokens = []
    text = f_text.split ("\n")
    for i in range(len(text)):
        temp = text[i].split(" ")
        for j in range(len(temp)):
            temp_1 = [0] * 3
            temp_1[0] = "Sentence: "+str(i)
            temp_1[1] = temp[j]
            if (i,j) in sentence_word_tag_map:
                temp_1[2] = sentence_word_tag_map[(i,j)]
            else:
                temp_1[2] = "O"
            input_tokens.append(temp_1)
    #also return number of sentences
    return input_tokens, len(sentences_individual)

labels = {'EVIDENTIAL','OCCURRENCE','PROBLEM', 'TEST', 'TREATMENT', 'CLINICAL_DEPT', "DATE",
                            "DURATION", "DISCHARGE", "ADMISSION", "TIME", "FREQUENCY"}
directory = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
#directory = os.fsencode(directory)
count_file, count_sentences, input_tokens_final = 0, 0, []
for file in os.listdir(directory):
    #print (file)
    filename = os.fsdecode(file)
    #print ("niorhgr", filename)
    if filename.endswith(".txt"):
        count_file += 1
        filename_ann = filename[:filename.find(".")] + ".xml.extent"
        input_tokens, offset = test_file_process(filename, filename_ann, labels)        
        input_tokens_final += process_input_tokens(input_tokens, count_sentences)
        count_sentences += offset

test_data = pd.DataFrame(input_tokens_final,columns=['Sentence#','Word','Tag'])
# data = data.sample(frac=1)
#test_data = preprocessing(test_data)
test_data.head(30)

getter = SentenceGetter(test_data)

test_sentences = [" ".join([s[0] for s in sent]) for sent in getter.sentences]
test_labels = [[s[1] for s in sent] for sent in getter.sentences]


test_tags_vals = list(set(test_data["Tag"].values))
test_tag2idx = {t: i for i, t in enumerate(test_tags_vals)}

test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]
test_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts],
                          maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_labels],
                     maxlen=MAX_LEN, value=tag2idx["O"], padding="post",
                     dtype="long", truncating="post")
test_masks=  [[float(i>0) for i in ii] for ii in test_inputs]
test_inputs = torch.tensor(test_inputs).to(torch.int64)
test_tags = torch.tensor(test_tags).to(torch.int64)
test_masks = torch.tensor(test_masks).to(torch.int64)


test_data = TensorDataset(test_inputs, test_masks, test_tags)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=valid_sampler, batch_size=bs)

model.eval()
eval_loss, eval_accuracy = 0, 0
nb_eval_steps, nb_eval_examples = 0, 0
predictions , true_labels = [], []
for batch in test_dataloader:
    batch = tuple(t.to(device) for t in batch)
    b_input_ids, b_input_mask, b_labels = batch
    
    with torch.no_grad():
        tmp_eval_loss = model(b_input_ids, token_type_ids=None,
                              attention_mask=b_input_mask, labels=b_labels)
        logits = model(b_input_ids, token_type_ids=None,
                       attention_mask=b_input_mask)
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()
    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
    true_labels.append(label_ids)
    
    tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    
    eval_loss += tmp_eval_loss.mean().item()
    eval_accuracy += tmp_eval_accuracy
    
    nb_eval_examples += b_input_ids.size(0)
    nb_eval_steps += 1
eval_loss = eval_loss/nb_eval_steps
print("Test loss: {}".format(eval_loss))
print("Test Accuracy: {}".format(eval_accuracy/nb_eval_steps))
pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]
valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]
print("Test F1-Score: {}".format(f1_score(pred_tags, valid_tags)))

import sklearn
classification_report = sklearn.metrics.classification_report(valid_tags, pred_tags)
print(classification_report)


# #EVALUATION ON TIMEX

# def timex_sentence_processing (sentence):
#     # sentence = sentence.replace("/", " ")
#     # sentence = sentence.replace("-", " ")
#     # sentence = sentence.replace("(", " ")
#     # sentence = sentence.replace(")", " ")
#     # sentence = sentence.replace("#", " ")
#     # sentence = sentence.replace("..", " .")
#     # sentence = sentence.replace("...", "  .")
#     # sentence = sentence.replace("[", " ")
#     # sentence = sentence.replace("]", " ")
#     # sentence = sentence.replace("*", " ")
#     # sentence = sentence.replace("\"", " ")
        
#     sentence_map, temp_1 = {},{}
#     temp = sentence.split("||")    
#     start, end, event =  temp[0].split(" ")[-2], temp[0].split(" ")[-1], temp[0].split(" ")[:-2]    
#     for i in range(1, len(temp)):        
#         key, value = temp[i].split("=")
#         temp_1[key] = value.replace('\"', "")
#     sentence_id = start.split(":")[0]
#     start = start.split(":")[1].replace('\"', "")
#     end = end.split(":")[1].replace('\"', "")
#     event_1, type_of_event = [event[0].split("=")[1]], event[0].split("=")[0]
#     event_1[0] = event_1[0].replace('\"', "")    
#     for i in range(1, len(event)):
#         event_1.append(event[i].replace('\"', ""))    
#     temp_1["start"] = start
#     temp_1["end"] = end
#     temp_1["event"] = " ".join(event_1)
#     sentence_map[sentence_id] = temp_1
#     if type_of_event == "TIMEX3":
#         return sentence_map   
#     else: 
#         return 0




# def test_file_process (filename, filename_ann, labels):
#     #path = "/content/drive/My Drive/NLP Project/Dropbox files/training data/"
#     path = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
#     f_text = open(os.path.expanduser(path + filename)).read()
#     f_ann = open(os.path.expanduser(path + filename_ann)).read()
#     sent_tokenize_list = sent_tokenize(f_ann)
#     sentences_individual = f_ann.split("\n")[:-1]
#     sentence_mapping = []
#     for i in sentences_individual:
#         output = timex_sentence_processing(i)
#         if output != 0:
#         #     print(i)
#         # else:
#             sentence_mapping.append(output)

#     final_sentence_mapping = []
#     for sentence in sentence_mapping:    
#         for key, value in sentence.items():
#             if value["type"] in labels:
#                 final_sentence_mapping.append(sentence)

#     all_bio_tags = []
#     for i in list(labels):
#         all_bio_tags.append("i-"+i)
#         all_bio_tags.append("b-"+i)
#     all_bio_tags.append("O")

#     sentence_word_tag_map = {}
#     for i in final_sentence_mapping:
#         for key, value in i.items():
#             sentence = int(key) - 1
#             start,end = int(value["start"]), int(value["end"])
#             if end - start == 0:
#                 sentence_word_tag_map[(sentence, start)] = "b-" + value["type"]
#             else:
#                 temp = value["event"].split(" ")
#                 for i in range(len(temp)):
#                     if i == 0:                    
#                         sentence_word_tag_map[(sentence,start+i)] = "b-" + value["type"]
#                     else:
#                         sentence_word_tag_map[(sentence,start+i)] = "i-" + value["type"]

#     sentence_word_tag_map = dict(sorted(sentence_word_tag_map.items(), key = lambda x: x[0][0]))


#     input_tokens = []
#     text = f_text.split ("\n")
#     for i in range(len(text)):
#         temp = text[i].split(" ")
#         for j in range(len(temp)):
#             temp_1 = [0] * 3
#             temp_1[0] = "Sentence: "+str(i)
#             temp_1[1] = temp[j]
#             if (i,j) in sentence_word_tag_map:
#                 temp_1[2] = sentence_word_tag_map[(i,j)]
#             else:
#                 temp_1[2] = "O"
#             input_tokens.append(temp_1)
#     #also return number of sentences
#     return input_tokens, len(sentences_individual)

# labels = {'EVIDENTIAL','OCCURRENCE','PROBLEM', 'TEST', 'TREATMENT', 'CLINICAL_DEPT', "DATE",
#                             "DURATION", "DISCHARGE", "ADMISSION", "TIME", "FREQUENCY"}
# directory = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
# #directory = os.fsencode(directory)
# count_file, count_sentences, input_tokens_final = 0, 0, []
# for file in os.listdir(directory):
#     #print (file)
#     filename = os.fsdecode(file)
#     #print ("niorhgr", filename)
#     if filename.endswith(".txt"):
#         count_file += 1
#         filename_ann = filename[:filename.find(".")] + ".xml.extent"
#         input_tokens, offset = test_file_process(filename, filename_ann, labels)        
#         input_tokens_final += process_input_tokens(input_tokens, count_sentences)
#         count_sentences += offset


# test_data = pd.DataFrame(input_tokens_final,columns=['Sentence#','Word','Tag'])
# # data = data.sample(frac=1)
# #test_data = preprocessing(test_data)
# test_data.head(30)

# getter = SentenceGetter(test_data)

# test_sentences = [" ".join([s[0] for s in sent]) for sent in getter.sentences]
# test_labels = [[s[1] for s in sent] for sent in getter.sentences]


# test_tags_vals = list(set(test_data["Tag"].values))
# test_tag2idx = {t: i for i, t in enumerate(test_tags_vals)}

# test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]
# test_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts],
#                           maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
# test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_labels],
#                      maxlen=MAX_LEN, value=tag2idx["O"], padding="post",
#                      dtype="long", truncating="post")
# test_masks=  [[float(i>0) for i in ii] for ii in test_inputs]
# test_inputs = torch.tensor(test_inputs).to(torch.int64)
# test_tags = torch.tensor(test_tags).to(torch.int64)
# test_masks = torch.tensor(test_masks).to(torch.int64)


# test_data = TensorDataset(test_inputs, test_masks, test_tags)
# test_sampler = SequentialSampler(test_data)
# test_dataloader = DataLoader(test_data, sampler=valid_sampler, batch_size=bs)

# model.eval()
# eval_loss, eval_accuracy = 0, 0
# nb_eval_steps, nb_eval_examples = 0, 0
# predictions , true_labels = [], []
# for batch in test_dataloader:
#     batch = tuple(t.to(device) for t in batch)
#     b_input_ids, b_input_mask, b_labels = batch
    
#     with torch.no_grad():
#         tmp_eval_loss = model(b_input_ids, token_type_ids=None,
#                               attention_mask=b_input_mask, labels=b_labels)
#         logits = model(b_input_ids, token_type_ids=None,
#                         attention_mask=b_input_mask)
#     logits = logits.detach().cpu().numpy()
#     label_ids = b_labels.to('cpu').numpy()
#     predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
#     true_labels.append(label_ids)
    
#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    
#     eval_loss += tmp_eval_loss.mean().item()
#     eval_accuracy += tmp_eval_accuracy
    
#     nb_eval_examples += b_input_ids.size(0)
#     nb_eval_steps += 1
# eval_loss = eval_loss/nb_eval_steps
# print("Test loss: {}".format(eval_loss))
# print("Test Accuracy: {}".format(eval_accuracy/nb_eval_steps))
# pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]
# valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]
# print("Test F1-Score: {}".format(f1_score(pred_tags, valid_tags)))

# import sklearn
# classification_report = sklearn.metrics.classification_report(valid_tags, pred_tags)
# print(classification_report)


# #EVALUATION ON SECTIME

# def sectime_sentence_processing (sentence):
#     # sentence = sentence.replace("/", " ")
#     # sentence = sentence.replace("-", " ")
#     # sentence = sentence.replace("(", " ")
#     # sentence = sentence.replace(")", " ")
#     # sentence = sentence.replace("#", " ")
#     # sentence = sentence.replace("..", " .")
#     # sentence = sentence.replace("...", "  .")
#     # sentence = sentence.replace("[", " ")
#     # sentence = sentence.replace("]", " ")
#     # sentence = sentence.replace("*", " ")
#     # sentence = sentence.replace("\"", " ")
        
#     sentence_map, temp_1 = {},{}
#     temp = sentence.split("||")    
#     start, end, event =  temp[0].split(" ")[-2], temp[0].split(" ")[-1], temp[0].split(" ")[:-2]    
#     for i in range(1, len(temp)):        
#         key, value = temp[i].split("=")
#         temp_1[key] = value.replace('\"', "")
#     sentence_id = start.split(":")[0]
#     start = start.split(":")[1].replace('\"', "")
#     end = end.split(":")[1].replace('\"', "")
#     event_1, type_of_event = [event[0].split("=")[1]], event[0].split("=")[0]
#     event_1[0] = event_1[0].replace('\"', "")    
#     for i in range(1, len(event)):
#         event_1.append(event[i].replace('\"', ""))    
#     temp_1["start"] = start
#     temp_1["end"] = end
#     temp_1["event"] = " ".join(event_1)
#     sentence_map[sentence_id] = temp_1
#     if type_of_event == "SECTIME":
#         return sentence_map   
#     else: 
#         return 0




# def test_file_process (filename, filename_ann, labels):
#     #path = "/content/drive/My Drive/NLP Project/Dropbox files/training data/"
#     path = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
#     f_text = open(os.path.expanduser(path + filename)).read()
#     f_ann = open(os.path.expanduser(path + filename_ann)).read()
#     sent_tokenize_list = sent_tokenize(f_ann)
#     sentences_individual = f_ann.split("\n")[:-1]
#     sentence_mapping = []
#     for i in sentences_individual:
#         output = sectime_sentence_processing(i)
#         if output != 0:
#         #     print(i)
#         # else:
#             sentence_mapping.append(output)

#     final_sentence_mapping = []
#     for sentence in sentence_mapping:    
#         for key, value in sentence.items():
#             if value["type"] in labels:
#                 final_sentence_mapping.append(sentence)

#     all_bio_tags = []
#     for i in list(labels):
#         all_bio_tags.append("i-"+i)
#         all_bio_tags.append("b-"+i)
#     all_bio_tags.append("O")

#     sentence_word_tag_map = {}
#     for i in final_sentence_mapping:
#         for key, value in i.items():
#             sentence = int(key) - 1
#             start,end = int(value["start"]), int(value["end"])
#             if end - start == 0:
#                 sentence_word_tag_map[(sentence, start)] = "b-" + value["type"]
#             else:
#                 temp = value["event"].split(" ")
#                 for i in range(len(temp)):
#                     if i == 0:                    
#                         sentence_word_tag_map[(sentence,start+i)] = "b-" + value["type"]
#                     else:
#                         sentence_word_tag_map[(sentence,start+i)] = "i-" + value["type"]

#     sentence_word_tag_map = dict(sorted(sentence_word_tag_map.items(), key = lambda x: x[0][0]))


#     input_tokens = []
#     text = f_text.split ("\n")
#     for i in range(len(text)):
#         temp = text[i].split(" ")
#         for j in range(len(temp)):
#             temp_1 = [0] * 3
#             temp_1[0] = "Sentence: "+str(i)
#             temp_1[1] = temp[j]
#             if (i,j) in sentence_word_tag_map:
#                 temp_1[2] = sentence_word_tag_map[(i,j)]
#             else:
#                 temp_1[2] = "O"
#             input_tokens.append(temp_1)
#     #also return number of sentences
#     return input_tokens, len(sentences_individual)

# labels = {'EVIDENTIAL','OCCURRENCE','PROBLEM', 'TEST', 'TREATMENT', 'CLINICAL_DEPT', "DATE",
#                             "DURATION", "DISCHARGE", "ADMISSION", "TIME", "FREQUENCY"}
# directory = "/content/drive/Shared drives/nlp/ground_truth/merged_i2b2/"
# #directory = os.fsencode(directory)
# count_file, count_sentences, input_tokens_final = 0, 0, []
# for file in os.listdir(directory):
#     #print (file)
#     filename = os.fsdecode(file)
#     #print ("niorhgr", filename)
#     if filename.endswith(".txt"):
#         count_file += 1
#         filename_ann = filename[:filename.find(".")] + ".xml.extent"
#         input_tokens, offset = test_file_process(filename, filename_ann, labels)        
#         input_tokens_final += process_input_tokens(input_tokens, count_sentences)
#         count_sentences += offset


# test_data = pd.DataFrame(input_tokens_final,columns=['Sentence#','Word','Tag'])
# # data = data.sample(frac=1)
# #test_data = preprocessing(test_data)
# test_data.head(30)

# getter = SentenceGetter(test_data)

# test_sentences = [" ".join([s[0] for s in sent]) for sent in getter.sentences]
# test_labels = [[s[1] for s in sent] for sent in getter.sentences]


# test_tags_vals = list(set(test_data["Tag"].values))
# test_tag2idx = {t: i for i, t in enumerate(test_tags_vals)}

# test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]
# test_inputs = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts],
#                           maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
# test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_labels],
#                      maxlen=MAX_LEN, value=tag2idx["O"], padding="post",
#                      dtype="long", truncating="post")
# test_masks=  [[float(i>0) for i in ii] for ii in test_inputs]
# test_inputs = torch.tensor(test_inputs).to(torch.int64)
# test_tags = torch.tensor(test_tags).to(torch.int64)
# test_masks = torch.tensor(test_masks).to(torch.int64)


# test_data = TensorDataset(test_inputs, test_masks, test_tags)
# test_sampler = SequentialSampler(test_data)
# test_dataloader = DataLoader(test_data, sampler=valid_sampler, batch_size=bs)

# model.eval()
# eval_loss, eval_accuracy = 0, 0
# nb_eval_steps, nb_eval_examples = 0, 0
# predictions , true_labels = [], []
# for batch in test_dataloader:
#     batch = tuple(t.to(device) for t in batch)
#     b_input_ids, b_input_mask, b_labels = batch
    
#     with torch.no_grad():
#         tmp_eval_loss = model(b_input_ids, token_type_ids=None,
#                               attention_mask=b_input_mask, labels=b_labels)
#         logits = model(b_input_ids, token_type_ids=None,
#                         attention_mask=b_input_mask)
#     logits = logits.detach().cpu().numpy()
#     label_ids = b_labels.to('cpu').numpy()
#     predictions.extend([list(p) for p in np.argmax(logits, axis=2)])
#     true_labels.append(label_ids)
    
#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)
    
#     eval_loss += tmp_eval_loss.mean().item()
#     eval_accuracy += tmp_eval_accuracy
    
#     nb_eval_examples += b_input_ids.size(0)
#     nb_eval_steps += 1
# eval_loss = eval_loss/nb_eval_steps
# print("Test loss: {}".format(eval_loss))
# print("Test Accuracy: {}".format(eval_accuracy/nb_eval_steps))
# pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]
# valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]
# print("Test F1-Score: {}".format(f1_score(pred_tags, valid_tags)))

# import sklearn
# classification_report = sklearn.metrics.classification_report(valid_tags, pred_tags)
# print(classification_report)

